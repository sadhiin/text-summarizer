{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/text-summarizer/research\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/text-summarizer'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_path: Path\n",
    "    tokenizer_path: Path\n",
    "    metric_file_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.textSummarizer.constants import *\n",
    "from src.textSummarizer.utils import read_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_path=CONFIG_FILE_PATH, params_path=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "        \n",
    "        os.makedirs(self.config.model_evaluation.root_dir, exist_ok=True)\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        model_evaluation_config = self.config.model_evaluation\n",
    "        \n",
    "        return ModelEvaluationConfig(\n",
    "            root_dir=Path(model_evaluation_config.root_dir),\n",
    "            data_path=Path(model_evaluation_config.data_path),\n",
    "            model_path=Path(model_evaluation_config.model_path),\n",
    "            tokenizer_path=Path(model_evaluation_config.tokenizer_path),\n",
    "            metric_file_name=model_evaluation_config.metric_file_name\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "    def generate_batch_sized_chunks(self, list_of_elements, batch_size):\n",
    "\n",
    "        for i in range(0, len(list_of_elements), batch_size):\n",
    "            yield list_of_elements[i:i+batch_size]\n",
    "\n",
    "    def calculate_metric_on_test_data(self,dataset, metric, model, tokenizers,\n",
    "                                    batch_size=16, column_text='article', column_summary=\"summary\"):\n",
    "        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "        summary_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "        for article_batch, summary_batch in tqdm(zip(article_batches, summary_batches), total=len(article_batches)):\n",
    "            # inputs = tokenizers(article_batch, max_length=1024, return_tensors='pt', truncation=True, padding=True).to(self.device)\n",
    "            # outputs = model.generate(**inputs)\n",
    "            # predictions = tokenizers.batch_decode(outputs, skip_special_tokens=True)\n",
    "            # metric.add_batch(predictions=predictions, references=summary_batch)\n",
    "\n",
    "            inputs = self.tokenizer(article_batch, max_length=1024, truncation=True, padding=True, return_tensors='pt').to(self.device)\n",
    "\n",
    "            summaries = model.generate(input_ids = inputs[\"input_ids\"].to(self.device),\n",
    "                                    attention_mask=inputs['attention_mask'].to(self.device),\n",
    "                                    length_penalty=0.8, num_beams=9, max_length=128)\n",
    "            \"\"\" parameter for length penalty to avoid the too long sequence to generate the model \"\"\"\n",
    "\n",
    "            decode_summaries = [self.tokenizer.decode(s, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "                                for s in summaries]\n",
    "            decode_summaries = [d.replace('', ' ') for d in decode_summaries]\n",
    "            metric.add_batch(predictions=decode_summaries, references=summary_batch)\n",
    "\n",
    "        score = metric.compute()\n",
    "        return score\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n",
    "        self.dataset = load_from_disk(self.config.data_path)\n",
    "        \n",
    "        rouge_names = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "        rouge_metric = evaluate.load('rouge')\n",
    "        \n",
    "        score = self.calculate_metric_on_test_data(\n",
    "            self.dataset['train'][0:5],\n",
    "            rouge_metric, self.model, self.tokenizer, batch_size=16, column_text='dialogue', column_summary='summary')\n",
    "        print(\"Sore: \", score)\n",
    "        rouge_dict = {rn: score[rn] for rn in rouge_names}\n",
    "        df = pd.DataFrame(rouge_dict, index=['pegasus'])\n",
    "        \n",
    "        df.to_csv(self.config.metric_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:41<00:00, 41.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sore:  {'rouge1': 0.0024844720496894407, 'rouge2': 0.0, 'rougeL': 0.0024844720496894407, 'rougeLsum': 0.0024844720496894407}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "model_evaluation_config = config.get_model_evaluation_config()\n",
    "model_evaluation = ModelEvaluation(model_evaluation_config)\n",
    "model_evaluation.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
